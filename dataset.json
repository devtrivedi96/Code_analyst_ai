{
  "code_samples": [
    "import os\nimport sys\nimport argparse\nimport logging\n\n# Add the project root to the sys.path to allow absolute imports from src\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, project_root)\n\nfrom src.utils.logger import setup_logging, logger\nfrom src.utils.file_loader import load_code_from_file\nfrom src.utils.constants import DEFAULT_MODEL, REPORT_DIR\n\nfrom src.analyzer.syntax_checker import check_syntax\nfrom src.analyzer.quality_analyzer import analyze_quality\nfrom src.analyzer.ai_reviewer import review_code_with_ai\nfrom src.analyzer.report_generator import generate_report\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"AI Code Analyst application.\")\n    parser.add_argument(\n        \"code_file\",\n        type=str,\n        help=\"Path to the Python code file to analyze.\"\n    )\n    parser.add_argument(\n        \"--output_report\",\n        type=str,\n        default=None,\n        help=\"Optional path to save the analysis report. Defaults to 'report_<filename>.md' in a 'reports' directory.\"\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=DEFAULT_MODEL,\n        help=f\"AI model to use for review (default: {DEFAULT_MODEL}).\"\n    )\n\n    args = parser.parse_args()\n\n    setup_logging()\n    logger.info(f\"Starting AI Code Analysis for {args.code_file}\")\n\n    analysis_results = {\n        \"code_file\": args.code_file,\n        \"syntax_valid\": False,\n        \"syntax_error\": None,\n        \"quality_metrics\": {},\n        \"ai_review\": {}\n    }\n\n    code_content = None\n    try:\n        code_content = load_code_from_file(args.code_file)\n        logger.info(\"Code loaded successfully.\")\n    except (FileNotFoundError, IOError) as e:\n        logger.error(f\"Failed to load code file: {e}\")\n        sys.exit(1)\n\n    # 1. Syntax Check\n    logger.info(\"Performing syntax check...\")\n    try:\n        is_valid = check_syntax(code_content)\n        analysis_results[\"syntax_valid\"] = is_valid\n        if not is_valid:\n            # The check_syntax function logs the error, no need to duplicate\n            logger.error(\"Syntax check failed.\")\n            # For simplicity, if syntax fails, we might stop or flag prominently.\n            # For now, we'll continue to show other analysis results.\n    except Exception as e:\n        logger.error(f\"Error during syntax check: {e}\")\n        analysis_results[\"syntax_valid\"] = False\n        analysis_results[\"syntax_error\"] = str(e)\n\n    # 2. Quality Analysis\n    logger.info(\"Performing code quality analysis...\")\n    try:\n        quality_metrics = analyze_quality(code_content)\n        analysis_results[\"quality_metrics\"] = quality_metrics\n        logger.info(\"Code quality analysis complete.\")\n    except Exception as e:\n        logger.error(f\"Error during code quality analysis: {e}\")\n\n    # 3. AI Review\n    logger.info(f\"Performing AI code review using model: {args.model}...\")\n    try:\n        ai_review_results = review_code_with_ai(code_content, model_name=args.model)\n        analysis_results[\"ai_review\"] = ai_review_results\n        logger.info(\"AI code review complete.\")\n    except Exception as e:\n        logger.error(f\"Error during AI code review: {e}\")\n\n    # 4. Generate Report\n    logger.info(\"Generating analysis report...\")\n    if args.output_report:\n        output_file_path = args.output_report\n    else:\n        # Default report path: reports/report_filename.md\n        report_dir = os.path.join(os.getcwd(), REPORT_DIR)\n        os.makedirs(report_dir, exist_ok=True)\n        filename_without_ext = os.path.splitext(os.path.basename(args.code_file))[0]\n        output_file_path = os.path.join(report_dir, f\"report_{filename_without_ext}.md\")\n\n    try:\n        generate_report(analysis_results, output_file_path)\n        logger.info(f\"Report saved to: {output_file_path}\")\n    except Exception as e:\n        logger.error(f\"Failed to generate report: {e}\")\n        sys.exit(1)\n\n    logger.info(\"AI Code Analysis complete.\")\n\nif __name__ == \"__main__\":\n    main()",
    "import os\nimport sys\nimport logging\nfrom flask import Flask, render_template, request, jsonify\n\n# Add the project root to the sys.path\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\nsys.path.insert(0, project_root)\n\nfrom src.analyzer.syntax_checker import check_syntax\nfrom src.analyzer.quality_analyzer import analyze_quality\nfrom src.analyzer.ai_reviewer import review_code_with_ai\nfrom src.analyzer.logic_analyzer import LogicAnalyzer\nfrom src.analyzer.best_practices import BestPracticesChecker\n\napp = Flask(__name__)\napp.config['JSON_SORT_KEYS'] = False\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n\n@app.route('/api/analyze', methods=['POST'])\ndef analyze_code():\n    \"\"\"Analyze code provided in request\"\"\"\n    try:\n        data = request.get_json()\n        code = data.get('code', '')\n        model = data.get('model', 'gemini-pro')\n\n        if not code.strip():\n            return jsonify({'error': 'No code provided'}), 400\n\n        # 1. Syntax Check\n        syntax_valid = check_syntax(code)\n        syntax_error = None\n        if not syntax_valid:\n            try:\n                compile(code, '<string>', 'exec')\n            except SyntaxError as e:\n                syntax_error = str(e)\n\n        # 2. Quality Analysis\n        quality_metrics = analyze_quality(code)\n\n        # 3. Logic Analysis\n        logic_analyzer = LogicAnalyzer()\n        logic_issues = logic_analyzer.analyze(code)\n        \n        # Convert list to proper response format\n        logic_analysis = {\n            'total_issues': len(logic_issues),\n            'issues': logic_issues,\n            'severity_count': {\n                'Critical': sum(1 for i in logic_issues if i.get('severity') == 'Critical'),\n                'Major': sum(1 for i in logic_issues if i.get('severity') == 'Major'),\n                'Minor': sum(1 for i in logic_issues if i.get('severity') == 'Minor')\n            }\n        }\n\n        # 4. Best Practices Check\n        practices_checker = BestPracticesChecker()\n        best_practices = practices_checker.check(code)\n\n        # 5. AI Review\n        ai_review = review_code_with_ai(code, model_name=model)\n\n        # Prepare response\n        analysis_results = {\n            'syntax_valid': syntax_valid,\n            'syntax_error': syntax_error,\n            'quality_metrics': quality_metrics,\n            'logic_analysis': logic_analysis,\n            'best_practices': best_practices,\n            'ai_review': ai_review\n        }\n\n        logger.info(\"Code analysis completed successfully\")\n        return jsonify(analysis_results), 200\n\n    except Exception as e:\n        logger.error(f\"Error during code analysis: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n\nif __name__ == '__main__':\n    # Only run locally, not on Vercel\n    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "SRC_DIR = \"src\"\nANALYZER_DIR = \"src/analyzer\"\nUTILS_DIR = \"src/utils\"\nMODELS_DIR = \"models\"\nPROMPTS_DIR = \"models/prompts\"\nDATA_DIR = \"data\"\nSAMPLE_CODE_DIR = \"data/sample_code\"\nREPORT_DIR = \"reports\"\nLOG_FILE = \"app.log\"\nDEFAULT_MODEL = \"gemini-pro\"",
    "import logging\nimport os\n\nfrom .constants import LOG_FILE, REPORT_DIR\n\n# Ensure the reports directory exists for logs\nlog_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', REPORT_DIR)\nos.makedirs(log_dir, exist_ok=True)\n\nlog_file_path = os.path.join(log_dir, LOG_FILE)\n\ndef setup_logging():\n    \"\"\"Sets up basic logging configuration.\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_file_path),  # Log to file\n            logging.StreamHandler()  # Log to console\n        ]\n    )\n    logging.info(\"Logging setup complete.\")\n\n# Initialize logging when this module is imported\nsetup_logging()\n\nlogger = logging.getLogger(__name__)",
    "import os\n\ndef load_code_from_file(filepath: str) -> str:\n    \"\"\"Loads code content from a specified file path.\"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found at: {filepath}\")\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            return f.read()\n    except Exception as e:\n        raise IOError(f\"Error reading file {filepath}: {e}\")",
    "#!/usr/bin/env python3\n\"\"\"\nCustom AI Model Integration Module\nSupports: CodeBERT training, local models, and fine-tuning\n\"\"\"\n\nimport os\nimport sys\nimport logging\nimport torch\nfrom typing import Dict, List, Tuple, Optional\nfrom pathlib import Path\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# ============================================================\n# 1. CodeBERT MODEL (microsoft/codebert-base)\n# ============================================================\n\nclass CodeBertAnalyzer:\n    \"\"\"Fine-tuned CodeBERT model for code analysis\"\"\"\n    \n    def __init__(self, model_name: str = \"microsoft/codebert-base\"):\n        \"\"\"Initialize CodeBERT model\"\"\"\n        try:\n            from transformers import AutoTokenizer, AutoModel\n            logger.info(f\"Loading CodeBERT model: {model_name}\")\n            \n            self.model_name = model_name\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModel.from_pretrained(model_name)\n            \n            # Move to GPU if available\n            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            self.model.to(self.device)\n            self.model.eval()\n            \n            logger.info(f\"\u2705 CodeBERT loaded successfully on {self.device}\")\n        except ImportError:\n            logger.error(\"Install transformers: pip install transformers torch\")\n            raise\n    \n    def analyze_code(self, code: str) -> Dict:\n        \"\"\"Analyze code using CodeBERT embeddings\"\"\"\n        try:\n            # Tokenize input code\n            inputs = self.tokenizer.encode(code, return_tensors=\"pt\", max_length=512, truncation=True)\n            inputs = inputs.to(self.device)\n            \n            # Get embeddings\n            with torch.no_grad():\n                outputs = self.model(inputs)\n                embeddings = outputs.last_hidden_state\n            \n            # Calculate statistics from embeddings\n            embedding_mean = embeddings.mean(dim=1).cpu().numpy()[0]\n            embedding_var = embeddings.var(dim=1).cpu().numpy()[0]\n            \n            # Generate insights\n            complexity_score = float(embedding_var.mean()) * 10  # 0-10 scale\n            \n            return {\n                \"model\": \"CodeBERT\",\n                \"complexity_score\": round(complexity_score, 2),\n                \"embedding_dim\": len(embedding_mean),\n                \"analysis\": \"CodeBERT deep learning analysis\",\n                \"insights\": self._generate_insights(complexity_score)\n            }\n        except Exception as e:\n            logger.error(f\"CodeBERT analysis error: {e}\")\n            return {\"error\": str(e)}\n    \n    def _generate_insights(self, score: float) -> List[str]:\n        \"\"\"Generate insights based on CodeBERT analysis\"\"\"\n        insights = []\n        \n        if score > 7:\n            insights.append(\"\ud83d\udd34 High complexity detected - Consider refactoring\")\n        elif score > 4:\n            insights.append(\"\ud83d\udfe1 Moderate complexity - Review structure\")\n        else:\n            insights.append(\"\ud83d\udfe2 Low complexity - Good code quality\")\n        \n        return insights\n    \n    def train_on_custom_data(self, code_samples: List[str], labels: List[int], \n                            epochs: int = 3, batch_size: int = 8):\n        \"\"\"Fine-tune CodeBERT on custom code samples\"\"\"\n        try:\n            from torch.utils.data import DataLoader, TensorDataset\n            from transformers import AdamW\n            \n            logger.info(f\"Fine-tuning CodeBERT on {len(code_samples)} samples...\")\n            \n            # Tokenize all samples\n            encoded_inputs = self.tokenizer(\n                code_samples,\n                padding=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            )\n            \n            # Create dataset\n            dataset = TensorDataset(\n                encoded_inputs['input_ids'],\n                encoded_inputs['attention_mask'],\n                torch.tensor(labels)\n            )\n            \n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n            optimizer = AdamW(self.model.parameters(), lr=2e-5)\n            \n            # Training loop\n            self.model.train()\n            for epoch in range(epochs):\n                total_loss = 0\n                for batch in dataloader:\n                    input_ids, attention_mask, batch_labels = batch\n                    input_ids = input_ids.to(self.device)\n                    attention_mask = attention_mask.to(self.device)\n                    batch_labels = batch_labels.to(self.device)\n                    \n                    optimizer.zero_grad()\n                    outputs = self.model(input_ids, attention_mask=attention_mask)\n                    \n                    # Simple MSE loss on embeddings mean\n                    loss = torch.nn.functional.mse_loss(\n                        outputs.last_hidden_state.mean(dim=1),\n                        batch_labels.float().unsqueeze(1)\n                    )\n                    \n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item()\n                \n                avg_loss = total_loss / len(dataloader)\n                logger.info(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n            \n            self.model.eval()\n            logger.info(\"\u2705 Fine-tuning completed!\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Training error: {e}\")\n            return False\n    \n    def save_model(self, path: str = \"./models/codebert-custom\"):\n        \"\"\"Save fine-tuned model\"\"\"\n        try:\n            Path(path).mkdir(parents=True, exist_ok=True)\n            self.model.save_pretrained(path)\n            self.tokenizer.save_pretrained(path)\n            logger.info(f\"\u2705 Model saved to {path}\")\n        except Exception as e:\n            logger.error(f\"Save error: {e}\")\n\n\n# ============================================================\n# 2. LOCAL MODELS (Ollama, LLaMA, etc.)\n# ============================================================\n\nclass LocalModelAnalyzer:\n    \"\"\"Interface for local running models\"\"\"\n    \n    def __init__(self, model_name: str = \"llama2\", base_url: str = \"http://localhost:11434\"):\n        \"\"\"Initialize local model connection\"\"\"\n        self.model_name = model_name\n        self.base_url = base_url\n        self.available = self._check_connection()\n    \n    def _check_connection(self) -> bool:\n        \"\"\"Check if local model is available\"\"\"\n        try:\n            import requests\n            response = requests.get(f\"{self.base_url}/api/tags\", timeout=2)\n            if response.status_code == 200:\n                logger.info(f\"\u2705 Connected to local model server at {self.base_url}\")\n                return True\n        except:\n            logger.warning(f\"\u26a0\ufe0f Local model server not running at {self.base_url}\")\n            return False\n    \n    def analyze_code(self, code: str) -> Dict:\n        \"\"\"Analyze code using local model\"\"\"\n        if not self.available:\n            return {\"error\": \"Local model server not available\"}\n        \n        try:\n            import requests\n            \n            prompt = f\"\"\"Analyze this Python code and provide:\n1. Summary\n2. Improvements\n3. Issues\n4. Rating (1-10)\n\nCode:\n```python\n{code}\n```\n\nProvide a concise analysis.\"\"\"\n            \n            response = requests.post(\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": self.model_name,\n                    \"prompt\": prompt,\n                    \"stream\": False,\n                    \"temperature\": 0.7\n                },\n                timeout=60\n            )\n            \n            if response.status_code == 200:\n                result = response.json()\n                return {\n                    \"model\": self.model_name,\n                    \"analysis\": result.get(\"response\", \"\"),\n                    \"source\": \"Local Model\"\n                }\n            else:\n                return {\"error\": f\"Model error: {response.status_code}\"}\n                \n        except Exception as e:\n            logger.error(f\"Local model analysis error: {e}\")\n            return {\"error\": str(e)}\n    \n    @staticmethod\n    def get_available_models() -> List[str]:\n        \"\"\"List available local models\"\"\"\n        try:\n            import requests\n            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n            if response.status_code == 200:\n                models = response.json().get(\"models\", [])\n                return [m[\"name\"] for m in models]\n        except:\n            pass\n        return []\n\n\n# ============================================================\n# 3. CUSTOM MODEL TRAINER\n# ============================================================\n\nclass CustomModelTrainer:\n    \"\"\"Train custom models on your code dataset\"\"\"\n    \n    def __init__(self, base_model: str = \"microsoft/codebert-base\"):\n        \"\"\"Initialize trainer\"\"\"\n        self.base_model = base_model\n        self.codebert = CodeBertAnalyzer(base_model)\n        self.training_history = []\n    \n    def prepare_dataset(self, code_files: List[str], \n                       labels: Optional[List[int]] = None) -> Tuple[List[str], List[int]]:\n        \"\"\"Prepare dataset from code files\"\"\"\n        code_samples = []\n        \n        for file_path in code_files:\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    code = f.read()\n                    if len(code.strip()) > 0:\n                        code_samples.append(code)\n            except Exception as e:\n                logger.warning(f\"Could not read {file_path}: {e}\")\n        \n        # Auto-generate labels if not provided\n        if labels is None:\n            labels = [len(code.split('\\n')) % 10 for code in code_samples]\n        \n        logger.info(f\"\ud83d\udcca Prepared {len(code_samples)} samples\")\n        return code_samples, labels\n    \n    def train(self, code_samples: List[str], labels: List[int], \n             epochs: int = 3, batch_size: int = 8):\n        \"\"\"Train the model\"\"\"\n        logger.info(f\"\ud83d\ude80 Starting training with {epochs} epochs...\")\n        \n        success = self.codebert.train_on_custom_data(\n            code_samples, labels, epochs, batch_size\n        )\n        \n        if success:\n            self.training_history.append({\n                \"samples\": len(code_samples),\n                \"epochs\": epochs,\n                \"status\": \"completed\"\n            })\n        \n        return success\n    \n    def save_trained_model(self, path: str):\n        \"\"\"Save the trained model\"\"\"\n        self.codebert.save_model(path)\n        logger.info(f\"\ud83d\udcbe Model saved to {path}\")\n\n\n# ============================================================\n# 4. UNIFIED ANALYZER (Combines all models)\n# ============================================================\n\nclass UnifiedCodeAnalyzer:\n    \"\"\"Combines CodeBERT, local models, and Gemini\"\"\"\n    \n    def __init__(self, use_codebert: bool = True, use_local: bool = False, use_gemini: bool = True):\n        \"\"\"Initialize unified analyzer\"\"\"\n        self.codebert = None\n        self.local_model = None\n        self.gemini_key = os.getenv('GEMINI_API_KEY')\n        \n        if use_codebert:\n            try:\n                self.codebert = CodeBertAnalyzer()\n            except:\n                logger.warning(\"CodeBERT not available\")\n        \n        if use_local:\n            self.local_model = LocalModelAnalyzer()\n        \n        self.use_gemini = use_gemini\n    \n    def analyze_code(self, code: str) -> Dict:\n        \"\"\"Comprehensive analysis using all available models\"\"\"\n        results = {\n            \"code_snippet\": code[:100] + \"...\" if len(code) > 100 else code,\n            \"models_used\": [],\n            \"analyses\": {}\n        }\n        \n        # CodeBERT analysis\n        if self.codebert:\n            try:\n                codebert_result = self.codebert.analyze_code(code)\n                results[\"analyses\"][\"codebert\"] = codebert_result\n                results[\"models_used\"].append(\"CodeBERT\")\n            except Exception as e:\n                logger.error(f\"CodeBERT failed: {e}\")\n        \n        # Local model analysis\n        if self.local_model and self.local_model.available:\n            try:\n                local_result = self.local_model.analyze_code(code)\n                results[\"analyses\"][\"local\"] = local_result\n                results[\"models_used\"].append(self.local_model.model_name)\n            except Exception as e:\n                logger.error(f\"Local model failed: {e}\")\n        \n        # Gemini analysis\n        if self.use_gemini and self.gemini_key:\n            try:\n                from src.analyzer.ai_reviewer import _review_with_gemini\n                gemini_result = _review_with_gemini(code)\n                results[\"analyses\"][\"gemini\"] = gemini_result\n                results[\"models_used\"].append(\"Gemini\")\n            except Exception as e:\n                logger.error(f\"Gemini failed: {e}\")\n        \n        results[\"total_models\"] = len(results[\"models_used\"])\n        return results\n    \n    def comparative_analysis(self, code: str) -> Dict:\n        \"\"\"Compare all models side-by-side\"\"\"\n        return {\n            \"unified_analysis\": self.analyze_code(code),\n            \"timestamp\": str(__import__('datetime').datetime.now()),\n            \"recommendation\": self._get_recommendation(code)\n        }\n    \n    def _get_recommendation(self, code: str) -> str:\n        \"\"\"Generate recommendation based on all analyses\"\"\"\n        recommendation = \"\u2705 Based on comprehensive multi-model analysis:\\n\"\n        \n        if self.codebert:\n            recommendation += \"- CodeBERT: Deep learning code embeddings analyzed\\n\"\n        if self.local_model and self.local_model.available:\n            recommendation += f\"- Local Model ({self.local_model.model_name}): Local inference complete\\n\"\n        if self.use_gemini:\n            recommendation += \"- Gemini: Cloud AI insights included\\n\"\n        \n        recommendation += \"\\nCombine insights from all models for best results!\"\n        return recommendation\n\n\n# ============================================================\n# 5. SETUP & INSTALLATION HELPERS\n# ============================================================\n\ndef install_dependencies():\n    \"\"\"Install required packages\"\"\"\n    packages = [\n        \"transformers>=4.30.0\",\n        \"torch>=2.0.0\",\n        \"requests>=2.28.0\",\n        \"numpy>=1.24.0\"\n    ]\n    \n    print(\"\ud83d\udce6 Installing required packages...\")\n    for package in packages:\n        print(f\"  Installing {package}...\")\n        os.system(f\"pip install -q {package}\")\n    print(\"\u2705 Dependencies installed!\")\n\n\ndef setup_codebert():\n    \"\"\"Download and setup CodeBERT\"\"\"\n    print(\"\ud83d\udce5 Setting up CodeBERT...\")\n    print(\"This will download ~350MB model file...\")\n    \n    try:\n        codebert = CodeBertAnalyzer()\n        print(\"\u2705 CodeBERT ready!\")\n        return codebert\n    except Exception as e:\n        print(f\"\u274c Error: {e}\")\n        return None\n\n\ndef setup_local_models():\n    \"\"\"Instructions for setting up local models\"\"\"\n    instructions = \"\"\"\n    \ud83d\udcd6 To use local models (Ollama):\n    \n    1. Install Ollama: https://ollama.ai\n    \n    2. Run locally:\n       ollama serve\n    \n    3. Download a model:\n       ollama pull llama2        # 3.8GB\n       ollama pull codellama     # 3.3GB (code-specific)\n       ollama pull mistral       # 2.6GB (fast)\n    \n    4. Models will run on http://localhost:11434\n    \n    Available models:\n    - llama2: Fast, general purpose\n    - codellama: Optimized for code\n    - mistral: Very fast\n    - neural-chat: Conversational\n    \"\"\"\n    print(instructions)\n\n\n# ============================================================\n# 6. EXAMPLE USAGE\n# ============================================================\n\ndef example_usage():\n    \"\"\"Example of using the unified analyzer\"\"\"\n    \n    # Sample code to analyze\n    sample_code = \"\"\"\ndef calculate_total(items):\n    total = 0\n    for item in items:\n        total += item['price'] * item['quantity']\n    return total\n\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83e\udd16 Multi-Model Code Analysis Example\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Initialize unified analyzer\n    print(\"\ud83d\ude80 Initializing analyzer with multiple models...\\n\")\n    analyzer = UnifiedCodeAnalyzer(\n        use_codebert=True,\n        use_local=True,\n        use_gemini=True\n    )\n    \n    # Run analysis\n    print(f\"\ud83d\udcdd Analyzing code:\\n{sample_code}\\n\")\n    results = analyzer.comparative_analysis(sample_code)\n    \n    print(\"\u2705 Analysis complete!\\n\")\n    import json\n    print(json.dumps(results, indent=2, default=str))\n\n\nif __name__ == \"__main__\":\n    # Check arguments\n    if len(sys.argv) > 1:\n        if sys.argv[1] == \"install\":\n            install_dependencies()\n        elif sys.argv[1] == \"setup-codebert\":\n            setup_codebert()\n        elif sys.argv[1] == \"setup-local\":\n            setup_local_models()\n        elif sys.argv[1] == \"example\":\n            example_usage()\n    else:\n        print(\"\"\"\n\ud83e\udd16 Custom AI Model Trainer\n        \nUsage:\n  python custom_models.py install       # Install dependencies\n  python custom_models.py setup-codebert # Setup CodeBERT\n  python custom_models.py setup-local    # Setup local models (Ollama)\n  python custom_models.py example        # Run example\n        \"\"\")\n",
    "#!/usr/bin/env python3\n\"\"\"\nIntegration layer for custom models with Flask app\n\"\"\"\n\nimport logging\nfrom src.analyzer.custom_models import (\n    CodeBertAnalyzer,\n    LocalModelAnalyzer,\n    UnifiedCodeAnalyzer\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass CustomModelIntegration:\n    \"\"\"Integrate custom models into Flask analyzer\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize custom model integration\"\"\"\n        self.codebert = None\n        self.local_model = None\n        self.unified_analyzer = None\n        self._initialize_models()\n    \n    def _initialize_models(self):\n        \"\"\"Initialize available models\"\"\"\n        # Try CodeBERT\n        try:\n            self.codebert = CodeBertAnalyzer()\n            logger.info(\"\u2705 CodeBERT loaded\")\n        except Exception as e:\n            logger.warning(f\"CodeBERT unavailable: {e}\")\n        \n        # Try local models\n        try:\n            self.local_model = LocalModelAnalyzer()\n            if self.local_model.available:\n                logger.info(f\"\u2705 Local model ({self.local_model.model_name}) available\")\n        except Exception as e:\n            logger.warning(f\"Local models unavailable: {e}\")\n        \n        # Initialize unified analyzer\n        try:\n            self.unified_analyzer = UnifiedCodeAnalyzer(\n                use_codebert=bool(self.codebert),\n                use_local=self.local_model.available if self.local_model else False,\n                use_gemini=True\n            )\n            logger.info(\"\u2705 Unified analyzer initialized\")\n        except Exception as e:\n            logger.warning(f\"Unified analyzer error: {e}\")\n    \n    def get_available_models(self) -> list:\n        \"\"\"Get list of available custom models\"\"\"\n        models = []\n        \n        if self.codebert:\n            models.append({\n                \"id\": \"codebert\",\n                \"name\": \"CodeBERT (microsoft/codebert-base)\",\n                \"type\": \"transformer\",\n                \"description\": \"Deep code understanding with embeddings\"\n            })\n        \n        if self.local_model and self.local_model.available:\n            models.append({\n                \"id\": \"local\",\n                \"name\": f\"Local Model ({self.local_model.model_name})\",\n                \"type\": \"local\",\n                \"description\": \"Local running model (Ollama)\"\n            })\n        \n        models.append({\n            \"id\": \"unified\",\n            \"name\": \"Unified Analysis\",\n            \"type\": \"hybrid\",\n            \"description\": \"Combines all available models\"\n        })\n        \n        return models\n    \n    def analyze_with_model(self, code: str, model_id: str) -> dict:\n        \"\"\"Analyze code with specific custom model\"\"\"\n        \n        if model_id == \"codebert\" and self.codebert:\n            return {\n                \"model\": \"CodeBERT\",\n                \"result\": self.codebert.analyze_code(code),\n                \"status\": \"success\"\n            }\n        \n        elif model_id == \"local\" and self.local_model and self.local_model.available:\n            return {\n                \"model\": \"Local Model\",\n                \"result\": self.local_model.analyze_code(code),\n                \"status\": \"success\"\n            }\n        \n        elif model_id == \"unified\" and self.unified_analyzer:\n            return {\n                \"model\": \"Unified Analysis\",\n                \"result\": self.unified_analyzer.comparative_analysis(code),\n                \"status\": \"success\"\n            }\n        \n        else:\n            return {\n                \"model\": model_id,\n                \"result\": None,\n                \"status\": \"error\",\n                \"message\": f\"Model {model_id} not available\"\n            }\n    \n    def train_codebert_on_code(self, code_samples: list, labels: list = None, \n                              epochs: int = 3, batch_size: int = 8) -> dict:\n        \"\"\"Fine-tune CodeBERT on custom code\"\"\"\n        if not self.codebert:\n            return {\n                \"status\": \"error\",\n                \"message\": \"CodeBERT not initialized\"\n            }\n        \n        try:\n            success = self.codebert.train_on_custom_data(\n                code_samples, labels, epochs, batch_size\n            )\n            return {\n                \"status\": \"success\" if success else \"error\",\n                \"samples\": len(code_samples),\n                \"epochs\": epochs,\n                \"message\": \"Training completed\" if success else \"Training failed\"\n            }\n        except Exception as e:\n            return {\n                \"status\": \"error\",\n                \"message\": str(e)\n            }\n    \n    def save_custom_model(self, path: str) -> dict:\n        \"\"\"Save trained model\"\"\"\n        if not self.codebert:\n            return {\n                \"status\": \"error\",\n                \"message\": \"CodeBERT not initialized\"\n            }\n        \n        try:\n            self.codebert.save_model(path)\n            return {\n                \"status\": \"success\",\n                \"path\": path,\n                \"message\": f\"Model saved to {path}\"\n            }\n        except Exception as e:\n            return {\n                \"status\": \"error\",\n                \"message\": str(e)\n            }\n\n\n# Singleton instance\n_integration = None\n\n\ndef get_custom_model_integration() -> CustomModelIntegration:\n    \"\"\"Get singleton instance\"\"\"\n    global _integration\n    if _integration is None:\n        _integration = CustomModelIntegration()\n    return _integration\n",
    "import logging\nfrom radon.complexity import cc_visit\n\nlogger = logging.getLogger(__name__)\n\ndef analyze_quality(code: str) -> dict:\n    \"\"\"Analyzes code quality metrics like line count and McCabe complexity.\n\n    Args:\n        code (str): The Python code string to analyze.\n\n    Returns:\n        dict: A dictionary containing 'line_count' and 'mccabe_complexity'.\n    \"\"\"\n    line_count = len(code.splitlines())\n    \n    mccabe_complexity = 0.0\n    try:\n        complexity_results = cc_visit(code)\n        if complexity_results:\n            total_complexity = sum(c.complexity for c in complexity_results)\n            mccabe_complexity = total_complexity / len(complexity_results)\n        else:\n            logger.info(\"No functions or classes found for McCabe complexity calculation.\")\n    except Exception as e:\n        logger.warning(f\"Could not calculate McCabe complexity: {e}\")\n\n    logger.info(f\"Code Quality Analysis: Line Count={line_count}, McCabe Complexity={mccabe_complexity:.2f}\")\n    \n    return {\n        \"line_count\": line_count,\n        \"mccabe_complexity\": round(mccabe_complexity, 2)\n    }",
    "import logging\nimport os\nimport google.generativeai as genai\n\nlogger = logging.getLogger(__name__)\n\n# Configure Gemini API\nGEMINI_API_KEY = \"AIzaSyAmAFvqu13MnqegONw1tvgFepmq-PZa2Zw\"\nif GEMINI_API_KEY:\n    genai.configure(api_key=GEMINI_API_KEY)\n    logger.info(\"Gemini API configured successfully\")\nelse:\n    logger.warning(\"GEMINI_API_KEY environment variable not set. AI review will use fallback.\")\n\ndef review_code_with_ai(code: str, model_name: str = \"gemini-pro\") -> dict:\n    \"\"\"Provides AI-driven code review using Google Gemini or other models.\n\n    Args:\n        code (str): The code string to review.\n        model_name (str): The name of the AI model to use.\n\n    Returns:\n        dict: A dictionary containing AI review result or fallback if API not available.\n    \"\"\"\n    \n    # Use Gemini if API key is available\n    if GEMINI_API_KEY and model_name == \"gemini-pro\":\n        return _review_with_gemini(code)\n    elif model_name == \"gpt-4\":\n        return _review_with_openai(code)\n    elif model_name == \"claude\":\n        return _review_with_claude(code)\n    else:\n        return _fallback_review(code, model_name)\n\n\ndef _review_with_gemini(code: str) -> dict:\n    \"\"\"Use Google Gemini API for code review.\"\"\"\n    try:\n        logger.info(\"Requesting AI review from Gemini...\")\n        \n        # Use the latest available Gemini model\n        model_names = ['gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash', 'gemini-pro']\n        model = None\n        used_model = None\n        \n        for model_name in model_names:\n            try:\n                model = genai.GenerativeModel(model_name)\n                used_model = model_name\n                logger.info(f\"Using model: {model_name}\")\n                break\n            except Exception as e:\n                logger.debug(f\"Model {model_name} not available: {e}\")\n                continue\n        \n        if not model:\n            logger.warning(\"No Gemini model available, using fallback\")\n            return _fallback_review(code, \"gemini-pro\")\n        \n        prompt = f\"\"\"Please review this Python code and provide:\n1. A brief summary of what the code does\n2. 3-5 specific improvement suggestions (be concise)\n3. Any potential bugs or issues\n4. Code quality rating (1-10)\n5. Overall recommendation\n\nCode to review:\n```python\n{code}\n```\n\nFormat your response as follows:\nSUMMARY: [brief summary]\nSUGGESTIONS: [bullet points]\nISSUES: [potential bugs or concerns]\nQUALITY_RATING: [1-10]\nRECOMMENDATION: [brief recommendation]\"\"\"\n\n        response = model.generate_content(prompt)\n        review_text = response.text\n        \n        # Parse the response\n        review_dict = _parse_gemini_response(review_text)\n        review_dict[\"model_used\"] = f\"Gemini {used_model.split('-')[-1].title()}\"\n        \n        logger.info(\"Gemini review completed successfully\")\n        return review_dict\n        \n    except Exception as e:\n        logger.error(f\"Gemini API error: {e}\")\n        return _fallback_review(code, \"gemini-pro\")\n\n\ndef _parse_gemini_response(text: str) -> dict:\n    \"\"\"Parse Gemini response into structured format.\"\"\"\n    sections = {\n        'summary': '',\n        'suggestions': [],\n        'issues': '',\n        'quality_rating': 'N/A',\n        'recommendation': '',\n    }\n    \n    lines = text.split('\\n')\n    current_section = None\n    \n    for line in lines:\n        line = line.strip()\n        \n        if line.startswith('SUMMARY:'):\n            sections['summary'] = line.replace('SUMMARY:', '').strip()\n        elif line.startswith('SUGGESTIONS:'):\n            current_section = 'suggestions'\n        elif line.startswith('ISSUES:'):\n            current_section = 'issues'\n            sections['issues'] = line.replace('ISSUES:', '').strip()\n        elif line.startswith('QUALITY_RATING:'):\n            sections['quality_rating'] = line.replace('QUALITY_RATING:', '').strip()\n        elif line.startswith('RECOMMENDATION:'):\n            sections['recommendation'] = line.replace('RECOMMENDATION:', '').strip()\n        elif line.startswith('- ') or line.startswith('\u2022 '):\n            if current_section == 'suggestions':\n                sections['suggestions'].append(line[2:].strip())\n    \n    return sections\n\n\ndef _review_with_openai(code: str) -> dict:\n    \"\"\"Use OpenAI GPT-4 API for code review.\"\"\"\n    try:\n        from openai import OpenAI\n        \n        logger.info(\"Requesting AI review from OpenAI...\")\n        \n        api_key = os.getenv('OPENAI_API_KEY')\n        if not api_key:\n            logger.warning(\"OPENAI_API_KEY not set, using fallback\")\n            return _fallback_review(code, \"gpt-4\")\n        \n        client = OpenAI(api_key=api_key)\n        \n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Review this Python code and provide:\n1. Summary of what it does\n2. 3-5 improvement suggestions\n3. Any bugs or issues\n4. Code quality rating (1-10)\n\nCode:\n```python\n{code}\n```\"\"\"\n                }\n            ],\n            temperature=0.7,\n            max_tokens=500\n        )\n        \n        review_text = response.choices[0].message.content\n        sections = _parse_gemini_response(review_text)\n        sections['model_used'] = 'GPT-4'\n        \n        logger.info(\"OpenAI review completed successfully\")\n        return sections\n        \n    except Exception as e:\n        logger.error(f\"OpenAI API error: {e}\")\n        return _fallback_review(code, \"gpt-4\")\n\n\ndef _review_with_claude(code: str) -> dict:\n    \"\"\"Use Anthropic Claude API for code review.\"\"\"\n    try:\n        import anthropic\n        \n        logger.info(\"Requesting AI review from Claude...\")\n        \n        api_key = os.getenv('ANTHROPIC_API_KEY')\n        if not api_key:\n            logger.warning(\"ANTHROPIC_API_KEY not set, using fallback\")\n            return _fallback_review(code, \"claude\")\n        \n        client = anthropic.Anthropic(api_key=api_key)\n        \n        message = client.messages.create(\n            model=\"claude-3-opus-20240229\",\n            max_tokens=500,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Review this Python code and provide:\n1. Summary of what it does\n2. 3-5 improvement suggestions\n3. Any bugs or issues\n4. Code quality rating (1-10)\n\nCode:\n```python\n{code}\n```\"\"\"\n                }\n            ]\n        )\n        \n        review_text = message.content[0].text\n        sections = _parse_gemini_response(review_text)\n        sections['model_used'] = 'Claude 3 Opus'\n        \n        logger.info(\"Claude review completed successfully\")\n        return sections\n        \n    except Exception as e:\n        logger.error(f\"Claude API error: {e}\")\n        return _fallback_review(code, \"claude\")\n\n\ndef _fallback_review(code: str, model_name: str) -> dict:\n    \"\"\"Fallback review when API is not available.\"\"\"\n    logger.info(f\"Using fallback review (API not available for {model_name})\")\n    \n    return {\n        \"summary\": \"AI review unavailable. Please set up your API key to get real AI-powered code review.\",\n        \"suggestions\": [\n            \"Set up GEMINI_API_KEY, OPENAI_API_KEY, or ANTHROPIC_API_KEY environment variable\",\n            \"Add comprehensive docstrings to functions and classes\",\n            \"Implement robust error handling and validation\",\n            \"Consider adding type hints for better code clarity\",\n            \"Break down complex functions into smaller, focused units\"\n        ],\n        \"issues\": \"API not configured\",\n        \"quality_rating\": \"Unable to rate\",\n        \"recommendation\": \"Configure API keys to enable real AI review\",\n        \"model_used\": f\"{model_name} (Fallback)\"\n    }",
    "import logging\nimport json\n\nlogger = logging.getLogger(__name__)\n\ndef generate_report(analysis_results: dict, output_file: str):\n    \"\"\"Generates a comprehensive report from analysis results and saves it to a file.\n\n    Args:\n        analysis_results (dict): A dictionary containing all analysis results.\n        output_file (str): The path to the file where the report will be saved.\n    \"\"\"\n    report_content = []\n    report_content.append(\"# AI Code Analysis Report\")\n    report_content.append(\"## Overview\")\n\n    # Add Syntax Check Results\n    syntax_status = \"Passed\" if analysis_results.get(\"syntax_valid\", False) else \"Failed\"\n    report_content.append(f\"- **Syntax Check**: {syntax_status}\")\n    if not analysis_results.get(\"syntax_valid\", False) and analysis_results.get(\"syntax_error\"):\n        report_content.append(f\"  * Error: {analysis_results['syntax_error']}\")\n\n    # Add Quality Analysis Results\n    quality_metrics = analysis_results.get(\"quality_metrics\", {})\n    report_content.append(\"## Code Quality Analysis\")\n    report_content.append(f\"- **Lines of Code**: {quality_metrics.get('line_count', 'N/A')}\")\n    report_content.append(f\"- **McCabe Complexity**: {quality_metrics.get('mccabe_complexity', 'N/A')}\")\n\n    # Add AI Review Results\n    ai_review = analysis_results.get(\"ai_review\", {})\n    report_content.append(\"## AI Review\")\n    report_content.append(f\"- **Summary**: {ai_review.get('summary', 'No AI review summary available.')}\")\n    if ai_review.get('suggestions'):\n        report_content.append(\"- **Suggestions**:\")\n        for suggestion in ai_review['suggestions']:\n            report_content.append(f\"  * {suggestion}\")\n    report_content.append(f\"- **Severity**: {ai_review.get('severity', 'N/A')}\")\n    report_content.append(f\"- **Model Used**: {ai_review.get('model_used', 'N/A')}\")\n\n    # Add Raw JSON for debugging/completeness\n    report_content.append(\"## Raw Analysis Data (JSON)\")\n    report_content.append(\"```json\")\n    report_content.append(json.dumps(analysis_results, indent=2))\n    report_content.append(\"```\")\n\n    final_report = \"\".join(report_content)\n\n    try:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(final_report)\n        logger.info(f\"Analysis report successfully generated and saved to {output_file}\")\n    except IOError as e:\n        logger.error(f\"Error writing report to {output_file}: {e}\")",
    "import logging\nimport re\nimport ast\nfrom typing import Dict, List, Tuple\n\nlogger = logging.getLogger(__name__)\n\n\nclass LogicAnalyzer:\n    \"\"\"Analyzes code logic for actual bugs and issues.\"\"\"\n\n    def __init__(self):\n        self.issues = []\n        self.suggestions = []\n\n    def analyze(self, code: str) -> List[Dict]:\n        \"\"\"Perform comprehensive logic analysis.\"\"\"\n        self.issues = []\n        self.suggestions = []\n\n        # Run ONLY logic-critical checks\n        self._check_division_by_zero(code)\n        self._check_infinite_loops(code)\n        self._check_undefined_variables(code)\n        self._check_logic_errors(code)\n        self._check_error_handling(code)\n        self._check_unreachable_code(code)\n        self._check_type_mismatches(code)\n\n        logger.info(f\"Logic analysis complete: {len(self.issues)} issues found\")\n\n        return self.issues\n\n    def _check_division_by_zero(self, code: str):\n        \"\"\"Check for potential division by zero.\"\"\"\n        lines = code.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            # Look for division operations with zero\n            if re.search(r'/\\s*0(?![0-9])', line) or re.search(r'/\\s*0\\.0', line):\n                self.issues.append({\n                    'line': i,\n                    'type': 'Division by Zero',\n                    'severity': 'Critical',\n                    'message': f\"Potential division by zero: {line.strip()}\",\n                    'suggestion': \"Check that the divisor is not zero before dividing. Use: if divisor != 0: result = a / divisor\"\n                })\n            \n            # Look for variable division that might be zero\n            if '/ ' in line and not '/ 0' in line:\n                # Check if variable could be zero\n                var_match = re.search(r'/\\s*(\\w+)', line)\n                if var_match:\n                    var_name = var_match.group(1)\n                    # Check if this variable is set to 0 anywhere\n                    for check_line in lines:\n                        if re.search(rf'{var_name}\\s*=\\s*0(?![0-9])', check_line):\n                            self.issues.append({\n                                'line': i,\n                                'type': 'Division by Zero Risk',\n                                'severity': 'Major',\n                                'message': f\"Variable '{var_name}' may be zero when dividing: {line.strip()}\",\n                                'suggestion': f\"Add validation: if {var_name} != 0: before division\"\n                            })\n                            break\n\n    def _check_infinite_loops(self, code: str):\n        \"\"\"Check for potential infinite loops.\"\"\"\n        lines = code.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            # Check for 'while True:' without break\n            if 'while True:' in line or 'while 1:' in line:\n                # Look ahead for break statement\n                has_break = False\n                for j in range(i, min(i + 20, len(lines))):\n                    if 'break' in lines[j]:\n                        has_break = True\n                        break\n                    if lines[j].strip() and not lines[j][0].isspace() and j > i:\n                        break\n                \n                if not has_break:\n                    self.issues.append({\n                        'line': i,\n                        'type': 'Infinite Loop',\n                        'severity': 'Critical',\n                        'message': f\"Infinite loop detected: while True without break\",\n                        'suggestion': \"Add a break condition: while True: ... if condition: break\"\n                    })\n            \n            # Check for incrementing loop variable\n            if 'for ' in line and 'range(' in line:\n                # Check if range(0) or range(-n)\n                range_match = re.search(r'range\\((-?\\d+)\\)', line)\n                if range_match:\n                    range_val = int(range_match.group(1))\n                    if range_val <= 0:\n                        self.issues.append({\n                            'line': i,\n                            'type': 'Infinite Loop Risk',\n                            'severity': 'Major',\n                            'message': f\"Loop range is {range_val} - will not execute or loop forever\",\n                            'suggestion': f\"Ensure range has positive value: range({abs(range_val) if range_val < 0 else 1})\"\n                        })\n\n    def _check_undefined_variables(self, code: str):\n        \"\"\"Check for potentially undefined variables.\"\"\"\n        try:\n            tree = ast.parse(code)\n        except SyntaxError:\n            return\n        \n        defined_vars = set()\n        used_vars = set()\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.Assign):\n                for target in node.targets:\n                    if isinstance(target, ast.Name):\n                        defined_vars.add(target.id)\n            elif isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):\n                used_vars.add(node.id)\n        \n        undefined = used_vars - defined_vars - {'print', 'len', 'range', 'str', 'int', 'float', 'list', 'dict', 'set', 'open', 'True', 'False', 'None'}\n        \n        for var in undefined:\n            lines = code.split('\\n')\n            for i, line in enumerate(lines, 1):\n                if var in line and 'def ' not in line:\n                    self.issues.append({\n                        'line': i,\n                        'type': 'Undefined Variable',\n                        'severity': 'Critical',\n                        'message': f\"Variable '{var}' may be undefined: {line.strip()}\",\n                        'suggestion': f\"Define '{var}' before using it: {var} = ...\"\n                    })\n                    break\n\n    def _check_logic_errors(self, code: str):\n        \"\"\"Check for logic errors and faulty conditions.\"\"\"\n        lines = code.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n            \n            # Check for always True/False conditions\n            if 'if True:' in line or 'if 1:' in line:\n                self.issues.append({\n                    'line': i,\n                    'type': 'Logic Error',\n                    'severity': 'Minor',\n                    'message': \"Condition is always True\",\n                    'suggestion': \"Replace with actual condition or remove if statement\"\n                })\n            \n            if 'if False:' in line or 'if 0:' in line or 'if None:' in line:\n                self.issues.append({\n                    'line': i,\n                    'type': 'Logic Error',\n                    'severity': 'Minor',\n                    'message': \"Condition is always False - this code will never execute\",\n                    'suggestion': \"Remove this if block or fix the condition\"\n                })\n            \n            # Check for assignment in condition\n            if re.search(r'if\\s+\\w+\\s*=\\s+', line):\n                self.issues.append({\n                    'line': i,\n                    'type': 'Logic Error',\n                    'severity': 'Major',\n                    'message': \"Assignment '=' used instead of comparison '==' in condition\",\n                    'suggestion': \"Use '==' for comparison: if variable == value:\"\n                })\n            \n            # Check for unreachable code after return\n            if stripped.startswith('return'):\n                if i < len(lines):\n                    next_line = lines[i].strip()\n                    if next_line and not next_line.startswith('#') and not next_line.startswith('def') and not next_line.startswith('class'):\n                        self.issues.append({\n                            'line': i + 1,\n                            'type': 'Unreachable Code',\n                            'severity': 'Major',\n                            'message': f\"Unreachable code after return: {next_line}\",\n                            'suggestion': \"Move this code before the return statement or remove it\"\n                        })\n\n    def _check_error_handling(self, code: str):\n        \"\"\"Check for missing error handling on risky operations.\"\"\"\n        lines = code.split('\\n')\n        dangerous_ops = [\n            ('open(', 'file operations'),\n            ('json.load', 'JSON parsing'),\n            ('requests.', 'network requests'),\n            ('int(', 'type conversion'),\n            ('float(', 'type conversion'),\n            ('[', 'list indexing'),\n        ]\n        \n        for i, line in enumerate(lines, 1):\n            if '#' in line:\n                line = line[:line.index('#')]\n            \n            for op, desc in dangerous_ops:\n                if op in line:\n                    # Check if in try block\n                    if i > 1 and 'try:' not in lines[i-2] and 'try:' not in lines[i-1]:\n                        if op == 'open(' or op == 'requests.' or op == 'json.load':\n                            self.issues.append({\n                                'line': i,\n                                'type': 'Missing Error Handling',\n                                'severity': 'Major',\n                                'message': f\"Missing error handling for {desc}: {line.strip()}\",\n                                'suggestion': f\"Wrap in try-except:\\ntry:\\n    {line.strip()}\\nexcept Exception as e:\\n    logger.error(f'Error: {{e}}')\"\n                            })\n\n    def _check_unreachable_code(self, code: str):\n        \"\"\"Check for unreachable code.\"\"\"\n        lines = code.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n            if stripped.startswith('return') or stripped.startswith('raise') or stripped == 'break' or stripped == 'continue':\n                # Check following lines for code\n                for j in range(i, min(i + 5, len(lines))):\n                    next_line = lines[j].strip()\n                    if next_line and not next_line.startswith('#'):\n                        if not (next_line.startswith('def ') or next_line.startswith('class ') or next_line.startswith('else') or next_line.startswith('except') or next_line.startswith('finally')):\n                            # Check indentation\n                            if len(lines[j]) - len(lines[j].lstrip()) <= len(line) - len(line.lstrip()):\n                                self.issues.append({\n                                    'line': j + 1,\n                                    'type': 'Unreachable Code',\n                                    'severity': 'Major',\n                                    'message': f\"Unreachable code: {next_line}\",\n                                    'suggestion': \"Move this code before the return/break statement or remove it\"\n                                })\n                                break\n\n    def _check_type_mismatches(self, code: str):\n        \"\"\"Check for potential type mismatches in operations.\"\"\"\n        lines = code.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n            if not stripped or stripped.startswith('#'):\n                continue\n            \n            # Check for string + number operations\n            # Pattern: variable + number or \"string\" + number\n            if '+' in line or '-' in line:\n                # Look for string literals being used with numbers\n                if re.search(r'[\"\\'].*[\"\\']?\\s*[\\+\\-]\\s*\\d', line):\n                    self.issues.append({\n                        'line': i,\n                        'type': 'Type Mismatch',\n                        'severity': 'Critical',\n                        'message': f\"Type mismatch: String and number operation: {line.strip()}\",\n                        'suggestion': \"Convert string to number first: int(age) + 5 or use f-strings: f'{age} is the age'\"\n                    })\n                \n                # Look for variable that is assigned as string being used with numbers\n                var_match = re.search(r'(\\w+)\\s*\\+\\s*\\d', line)\n                if var_match:\n                    var_name = var_match.group(1)\n                    # Check if this variable was assigned a string\n                    for check_line in lines:\n                        if re.search(rf'{var_name}\\s*=\\s*[\"\\']', check_line):\n                            self.issues.append({\n                                'line': i,\n                                'type': 'Type Mismatch',\n                                'severity': 'Critical',\n                                'message': f\"Type mismatch: '{var_name}' is a string but used in numeric operation: {line.strip()}\",\n                                'suggestion': f\"Convert to number: int({var_name}) + 5 or str(5) + {var_name}\"\n                            })\n                            break\n\n    def _count_severities(self) -> Dict[str, int]:\n\n        \"\"\"Count issues by severity.\"\"\"\n        counts = {'Critical': 0, 'Major': 0, 'Minor': 0}\n        for issue in self.issues:\n            severity = issue.get('severity', 'Minor')\n            if severity in counts:\n                counts[severity] += 1\n        return counts\n",
    "import logging\nimport re\n\nlogger = logging.getLogger(__name__)\n\n\nclass BestPracticesChecker:\n    \"\"\"Checks code against Python and software engineering best practices.\"\"\"\n\n    def check(self, code: str) -> dict:\n        \"\"\"Run all best practices checks.\"\"\"\n        practices = {\n            'pep8_violations': self._check_pep8(code),\n            'performance_issues': self._check_performance(code),\n            'security_issues': self._check_security(code),\n            'maintainability': self._check_maintainability(code),\n        }\n\n        logger.info(\"Best practices check complete\")\n        return practices\n\n    def _check_pep8(self, code: str) -> list:\n        \"\"\"Check PEP 8 style violations.\"\"\"\n        issues = []\n        lines = code.split('\\n')\n\n        for i, line in enumerate(lines, 1):\n            # Check line length\n            if len(line) > 79:\n                issues.append({\n                    'line': i,\n                    'issue': f'Line too long ({len(line)} > 79 characters)',\n                    'suggestion': 'Break long lines for readability'\n                })\n\n            # Check trailing whitespace\n            if line != line.rstrip():\n                issues.append({\n                    'line': i,\n                    'issue': 'Trailing whitespace',\n                    'suggestion': 'Remove trailing whitespace'\n                })\n\n            # Check multiple statements per line\n            if ';' in line and not line.strip().startswith('#'):\n                issues.append({\n                    'line': i,\n                    'issue': 'Multiple statements on one line',\n                    'suggestion': 'Put each statement on its own line'\n                })\n\n            # Check spacing around operators\n            if re.search(r'\\w\\s{2,}=\\s*\\w', line):\n                issues.append({\n                    'line': i,\n                    'issue': 'Inconsistent spacing around operators',\n                    'suggestion': 'Use single spaces around operators'\n                })\n\n        return issues\n\n    def _check_performance(self, code: str) -> list:\n        \"\"\"Check for performance issues.\"\"\"\n        issues = []\n\n        # Check for inefficient operations\n        if 'for ' in code and '.append(' in code:\n            if 'list(' not in code:\n                issues.append({\n                    'issue': 'Consider using list comprehension',\n                    'suggestion': 'Replace loop with list comprehension for better performance'\n                })\n\n        # Check for repeated function calls in loops\n        if 'for ' in code:\n            lines = code.split('\\n')\n            for i, line in enumerate(lines):\n                if 'len(' in line and 'range(len(' in line:\n                    issues.append({\n                        'line': i + 1,\n                        'issue': 'Inefficient range(len()) usage',\n                        'suggestion': 'Use enumerate() or direct iteration'\n                    })\n\n        # Check for N+1 query patterns\n        if 'for ' in code and 'if ' in code:\n            issues.append({\n                'issue': 'Potential N+1 query pattern',\n                'suggestion': 'Consider batching operations outside loops'\n            })\n\n        return issues\n\n    def _check_security(self, code: str) -> list:\n        \"\"\"Check for security vulnerabilities.\"\"\"\n        issues = []\n\n        # Check for SQL injection risk\n        if 'query' in code.lower() and '+' in code:\n            issues.append({\n                'issue': 'Potential SQL injection risk',\n                'suggestion': 'Use parameterized queries instead of string concatenation'\n            })\n\n        # Check for hardcoded credentials\n        if re.search(r'(password|api_key|secret)\\s*=\\s*[\"\\']', code, re.IGNORECASE):\n            issues.append({\n                'issue': 'Hardcoded credentials found',\n                'severity': 'Critical',\n                'suggestion': 'Use environment variables for sensitive data'\n            })\n\n        # Check for eval usage\n        if 'eval(' in code or 'exec(' in code:\n            issues.append({\n                'issue': 'Dangerous eval() or exec() usage',\n                'severity': 'Critical',\n                'suggestion': 'Avoid eval/exec. Use safer alternatives like ast.literal_eval()'\n            })\n\n        # Check for insecure file permissions\n        if 'chmod' in code and '0777' in code:\n            issues.append({\n                'issue': 'Insecure file permissions',\n                'suggestion': 'Use restrictive permissions like 0755 or 0644'\n            })\n\n        return issues\n\n    def _check_maintainability(self, code: str) -> list:\n        \"\"\"Check code maintainability.\"\"\"\n        issues = []\n\n        lines = code.split('\\n')\n\n        # Check for overly complex conditions\n        complex_conditions = [line for line in lines if line.count(' and ') > 3 or line.count(' or ') > 3]\n        if complex_conditions:\n            issues.append({\n                'issue': 'Complex conditional expressions',\n                'suggestion': 'Break complex conditions into variables or helper functions'\n            })\n\n        # Check for large functions\n        if 'def ' in code:\n            func_lines = 0\n            in_func = False\n            for line in lines:\n                if line.strip().startswith('def '):\n                    in_func = True\n                    func_lines = 0\n                elif in_func and (line and not line[0].isspace()):\n                    if func_lines > 50:\n                        issues.append({\n                            'issue': 'Large function detected',\n                            'suggestion': 'Consider breaking large functions into smaller, focused functions'\n                        })\n                    in_func = False\n                elif in_func:\n                    func_lines += 1\n\n        # Check for magic strings\n        if re.search(r'[\"\\'][a-zA-Z]{10,}[\"\\']', code):\n            issues.append({\n                'issue': 'Magic strings found',\n                'suggestion': 'Define string constants at module level'\n            })\n\n        return issues\n",
    "import ast\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef check_syntax(code: str) -> bool:\n    \"\"\"Checks the syntax of the given Python code string.\n\n    Args:\n        code (str): The Python code string to check.\n\n    Returns:\n        bool: True if syntax is valid, False otherwise.\n    \"\"\"\n    try:\n        ast.parse(code)\n        logger.info(\"Syntax check passed.\")\n        return True\n    except SyntaxError as e:\n        logger.error(f\"Syntax error found: {e}\")\n        return False"
  ],
  "labels": [
    4,
    8,
    0,
    7,
    1,
    1,
    2,
    3,
    9,
    3,
    8,
    0,
    1
  ],
  "metadata": {
    "source": "src",
    "count": 13,
    "file_types": [
      "python"
    ]
  }
}